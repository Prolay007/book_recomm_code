{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "412936a3-bef0-4c3e-92b8-dc4d821a8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchinfo\n",
    "import clip  # Import CLIP library\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from level_1_dataloader import image_dataloader\n",
    "from level_1_classifier_1 import level_1_classifier, level_1_model, level_1_output_layer\n",
    "from level_2_classifier import level_2_classifier, level_2_output_layer, level_2_pre_model_concate, level_2_post_model\n",
    "from Trainer_1 import Trainer_level_1, Trainer_level_2\n",
    "from Tester_1 import Tester_level_1, Tester_level_2\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffdf334-5b52-4efd-9db1-62baeca4570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = image_dataloader(csv_file='train_set.xls', root_dir='coverpage/coverpage/')\n",
    "val_set = image_dataloader(csv_file='val_set.xls', root_dir='coverpage/coverpage/')\n",
    "test_set = image_dataloader(csv_file='test_set.xls', root_dir='coverpage/coverpage/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6471cd4f-40be-4b24-bb27-52df05e2047b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of training sample 22340\n",
      "no. of validation sample 2804\n",
      "no. of testing sample 2809\n"
     ]
    }
   ],
   "source": [
    "print('no. of training sample', len(train_set))\n",
    "print('no. of validation sample', len(val_set))\n",
    "print('no. of testing sample', len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c689721-2b95-4f4d-8ca8-76aea68c1d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d5ef6b7-c25e-4756-93ab-6234ea075351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: pillow in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (10.3.0)\n",
      "Requirement already satisfied: filelock in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a88890c-37b3-458a-a1aa-a6c874323b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlavaForConditionalGeneration, AutoProcessor\n",
    "from torchinfo import summary\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "class VisionOnlyModel(torch.nn.Module):\n",
    "    def __init__(self, vision_model):\n",
    "        super(VisionOnlyModel, self).__init__()\n",
    "        self.vision_model = vision_model\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # Extract the class token (first token) from the last hidden state\n",
    "        outputs = self.vision_model(pixel_values=pixel_values).last_hidden_state\n",
    "        class_token_output = outputs[:, 0, :]  # Take the first token\n",
    "        return class_token_output\n",
    "\n",
    "def fine_tune_load_image_model():\n",
    "    # Load the pre-trained LLAVA model\n",
    "    llava_model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "    # Extract the vision model\n",
    "    vision_model = llava_model.vision_model\n",
    "\n",
    "    # Wrap it in a new model class\n",
    "    vision_only_model = VisionOnlyModel(vision_model)\n",
    "\n",
    "    # Enable gradients for the image model's parameters\n",
    "    for p in vision_only_model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return vision_only_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f138d19-b6e4-4323-bea7-e6a8cf835ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model = fine_tune_load_image_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51d9df7c-6ed7-4794-82eb-045d02d17241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99abc680f1dd40e5b263d969905f2e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaForConditionalGeneration(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (multi_modal_projector): LlavaMultiModalProjector(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELUActivation()\n",
      "    (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  )\n",
      "  (language_model): LlamaForCausalLM(\n",
      "    (model): LlamaModel(\n",
      "      (embed_tokens): Embedding(32064, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x LlamaDecoderLayer(\n",
      "          (self_attn): LlamaSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding()\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm()\n",
      "          (post_attention_layernorm): LlamaRMSNorm()\n",
      "        )\n",
      "      )\n",
      "      (norm): LlamaRMSNorm()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlavaForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained LLAVA model\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "# Print the model structure to find the vision model\n",
    "print(llava_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808f108d-88d8-4072-bf3a-10276d665a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlavaForConditionalGeneration, AutoProcessor\n",
    "from torchinfo import summary\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "class VisionOnlyModel(torch.nn.Module):\n",
    "    def __init__(self, vision_model):\n",
    "        super(VisionOnlyModel, self).__init__()\n",
    "        self.vision_model = vision_model\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # Extract the class token (first token) from the last hidden state\n",
    "        outputs = self.vision_model(pixel_values=pixel_values).last_hidden_state\n",
    "        class_token_output = outputs[:, 0, :]  # Take the first token\n",
    "        return class_token_output\n",
    "\n",
    "def fine_tune_load_image_model():\n",
    "    # Load the pre-trained LLAVA model\n",
    "    llava_model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "    # Extract the vision model\n",
    "    vision_model = llava_model.vision_tower.vision_model\n",
    "\n",
    "    # Wrap it in a new model class\n",
    "    vision_only_model = VisionOnlyModel(vision_model)\n",
    "\n",
    "    # Enable gradients for the image model's parameters\n",
    "    for p in vision_only_model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return vision_only_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e5ba585-5716-4dd7-8664-6fad3c1573fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170336c12dfa466492673d2f154ffada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vision_model = fine_tune_load_image_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa46f8a6-f9f8-406b-aae0-8db668c16035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5bb1cd82d547468f505bfd2aa17d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionOnlyModel(\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (position_embedding): Embedding(577, 1024)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "FE_model = fine_tune_load_image_model()\n",
    "print(FE_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a33f7343-8583-41d3-b06c-170cae59c622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================================================================================================\n",
      "Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
      "=========================================================================================================================================================================================\n",
      "VisionOnlyModel                                              [1, 3, 336, 336]          [1, 1024]                 --                        --                        --\n",
      "├─CLIPVisionTransformer: 1-1                                 --                        [1, 1024]                 --                        --                        --\n",
      "│    └─CLIPVisionEmbeddings: 2-1                             [1, 3, 336, 336]          [1, 577, 1024]            1,024                     --                        --\n",
      "│    │    └─Conv2d: 3-1                                      [1, 3, 336, 336]          [1, 1024, 24, 24]         602,112                   [14, 14]                  346,816,512\n",
      "│    │    └─Embedding: 3-2                                   [1, 577]                  [1, 577, 1024]            590,848                   --                        590,848\n",
      "│    └─LayerNorm: 2-2                                        [1, 577, 1024]            [1, 577, 1024]            2,048                     --                        2,048\n",
      "│    └─CLIPEncoder: 2-3                                      --                        [1, 577, 1024]            --                        --                        --\n",
      "│    │    └─ModuleList: 3-3                                  --                        --                        302,309,376               --                        --\n",
      "│    └─LayerNorm: 2-4                                        [1, 1024]                 [1, 1024]                 2,048                     --                        2,048\n",
      "=========================================================================================================================================================================================\n",
      "Total params: 303,507,456\n",
      "Trainable params: 303,507,456\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 649.72\n",
      "=========================================================================================================================================================================================\n",
      "Input size (MB): 1.35\n",
      "Forward/backward pass size (MB): 1262.05\n",
      "Params size (MB): 1214.03\n",
      "Estimated Total Size (MB): 2477.43\n",
      "=========================================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================================================================================================\n",
       "Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "=========================================================================================================================================================================================\n",
       "VisionOnlyModel                                              [1, 3, 336, 336]          [1, 1024]                 --                        --                        --\n",
       "├─CLIPVisionTransformer: 1-1                                 --                        [1, 1024]                 --                        --                        --\n",
       "│    └─CLIPVisionEmbeddings: 2-1                             [1, 3, 336, 336]          [1, 577, 1024]            1,024                     --                        --\n",
       "│    │    └─Conv2d: 3-1                                      [1, 3, 336, 336]          [1, 1024, 24, 24]         602,112                   [14, 14]                  346,816,512\n",
       "│    │    └─Embedding: 3-2                                   [1, 577]                  [1, 577, 1024]            590,848                   --                        590,848\n",
       "│    └─LayerNorm: 2-2                                        [1, 577, 1024]            [1, 577, 1024]            2,048                     --                        2,048\n",
       "│    └─CLIPEncoder: 2-3                                      --                        [1, 577, 1024]            --                        --                        --\n",
       "│    │    └─ModuleList: 3-3                                  --                        --                        302,309,376               --                        --\n",
       "│    └─LayerNorm: 2-4                                        [1, 1024]                 [1, 1024]                 2,048                     --                        2,048\n",
       "=========================================================================================================================================================================================\n",
       "Total params: 303,507,456\n",
       "Trainable params: 303,507,456\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 649.72\n",
       "=========================================================================================================================================================================================\n",
       "Input size (MB): 1.35\n",
       "Forward/backward pass size (MB): 1262.05\n",
       "Params size (MB): 1214.03\n",
       "Estimated Total Size (MB): 2477.43\n",
       "========================================================================================================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(\n",
    "    vision_model, \n",
    "    input_size=(1, 3, 336, 336), \n",
    "    col_names=('input_size', 'output_size', 'num_params', 'kernel_size', 'mult_adds'), \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8fe67af-048b-4f8d-9a5c-1b509ea0440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ./checkpoints/Llava/level_1/ is ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "used_model = 'Llava'\n",
    "used_model_feature_size = 1024\n",
    "level_1_checkpoint_dir = './checkpoints/' + used_model + '/level_1/'\n",
    "level_1_model_file = 'model.pth'\n",
    "level_1_csvlogger_file = 'log.csv'\n",
    "level_1_weights_path = os.path.join(level_1_checkpoint_dir, level_1_model_file)\n",
    "feature_size_extract_from_level_1 = 64\n",
    "no_epoch = 10\n",
    "lr = 0.00001\n",
    "optimizer = 'AdamW'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(level_1_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Print a message to confirm the directory creation (optional)\n",
    "print(f\"Directory {level_1_checkpoint_dir} is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3823f10e-0e74-44fc-b1fd-e7cf5572c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_model_feature_size = 1024  # Example value, adjust based on your model\n",
    "feature_size_extract_from_level_1 = 1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97e30de6-9b16-465c-aa8d-4fe7c9cc6e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and prepare model components\n",
    "classifier_level_1 = level_1_classifier(feature_size=used_model_feature_size, feature_size_extract_from_level_1=feature_size_extract_from_level_1)\n",
    "output_layer_level_1 = level_1_output_layer(feature_size_extract_from_level_1=feature_size_extract_from_level_1, no_class=2)\n",
    "model_level_1 = level_1_model(FE_model, classifier_level_1, output_layer_level_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "007d9eed-0baa-4f0b-9609-1ae1f956f78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================================================================================================\n",
      "Layer (type:depth-idx)                                            Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
      "==============================================================================================================================================================================================\n",
      "level_1_model                                                     [1, 3, 336, 336]          [1, 2]                    --                        --                        --\n",
      "├─VisionOnlyModel: 1-1                                            [1, 3, 336, 336]          [1, 1024]                 --                        --                        --\n",
      "│    └─CLIPVisionTransformer: 2-1                                 --                        [1, 1024]                 --                        --                        --\n",
      "│    │    └─CLIPVisionEmbeddings: 3-1                             [1, 3, 336, 336]          [1, 577, 1024]            1,193,984                 --                        347,407,360\n",
      "│    │    └─LayerNorm: 3-2                                        [1, 577, 1024]            [1, 577, 1024]            2,048                     --                        2,048\n",
      "│    │    └─CLIPEncoder: 3-3                                      --                        [1, 577, 1024]            302,309,376               --                        302,309,376\n",
      "│    │    └─LayerNorm: 3-4                                        [1, 1024]                 [1, 1024]                 2,048                     --                        2,048\n",
      "├─level_1_classifier: 1-2                                         [1, 1024]                 [1, 1024]                 --                        --                        --\n",
      "│    └─Linear: 2-2                                                [1, 1024]                 [1, 512]                  524,800                   --                        524,800\n",
      "│    └─ReLU: 2-3                                                  [1, 512]                  [1, 512]                  --                        --                        --\n",
      "│    └─Dropout: 2-4                                               [1, 512]                  [1, 512]                  --                        --                        --\n",
      "│    └─Linear: 2-5                                                [1, 512]                  [1, 256]                  131,328                   --                        131,328\n",
      "│    └─ReLU: 2-6                                                  [1, 256]                  [1, 256]                  --                        --                        --\n",
      "│    └─Dropout: 2-7                                               [1, 256]                  [1, 256]                  --                        --                        --\n",
      "│    └─Linear: 2-8                                                [1, 256]                  [1, 128]                  32,896                    --                        32,896\n",
      "│    └─ReLU: 2-9                                                  [1, 128]                  [1, 128]                  --                        --                        --\n",
      "│    └─Dropout: 2-10                                              [1, 128]                  [1, 128]                  --                        --                        --\n",
      "│    └─Linear: 2-11                                               [1, 128]                  [1, 1024]                 132,096                   --                        132,096\n",
      "│    └─ReLU: 2-12                                                 [1, 1024]                 [1, 1024]                 --                        --                        --\n",
      "│    └─Dropout: 2-13                                              [1, 1024]                 [1, 1024]                 --                        --                        --\n",
      "├─level_1_output_layer: 1-3                                       [1, 1024]                 [1, 2]                    --                        --                        --\n",
      "│    └─Linear: 2-14                                               [1, 1024]                 [1, 2]                    2,050                     --                        2,050\n",
      "│    └─Softmax: 2-15                                              [1, 2]                    [1, 2]                    --                        --                        --\n",
      "==============================================================================================================================================================================================\n",
      "Total params: 304,330,626\n",
      "Trainable params: 304,330,626\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 650.54\n",
      "==============================================================================================================================================================================================\n",
      "Input size (MB): 1.35\n",
      "Forward/backward pass size (MB): 1262.07\n",
      "Params size (MB): 1217.32\n",
      "Estimated Total Size (MB): 2480.74\n",
      "==============================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================================================================================================\n",
       "Layer (type:depth-idx)                                            Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "==============================================================================================================================================================================================\n",
       "level_1_model                                                     [1, 3, 336, 336]          [1, 2]                    --                        --                        --\n",
       "├─VisionOnlyModel: 1-1                                            [1, 3, 336, 336]          [1, 1024]                 --                        --                        --\n",
       "│    └─CLIPVisionTransformer: 2-1                                 --                        [1, 1024]                 --                        --                        --\n",
       "│    │    └─CLIPVisionEmbeddings: 3-1                             [1, 3, 336, 336]          [1, 577, 1024]            1,193,984                 --                        347,407,360\n",
       "│    │    └─LayerNorm: 3-2                                        [1, 577, 1024]            [1, 577, 1024]            2,048                     --                        2,048\n",
       "│    │    └─CLIPEncoder: 3-3                                      --                        [1, 577, 1024]            302,309,376               --                        302,309,376\n",
       "│    │    └─LayerNorm: 3-4                                        [1, 1024]                 [1, 1024]                 2,048                     --                        2,048\n",
       "├─level_1_classifier: 1-2                                         [1, 1024]                 [1, 1024]                 --                        --                        --\n",
       "│    └─Linear: 2-2                                                [1, 1024]                 [1, 512]                  524,800                   --                        524,800\n",
       "│    └─ReLU: 2-3                                                  [1, 512]                  [1, 512]                  --                        --                        --\n",
       "│    └─Dropout: 2-4                                               [1, 512]                  [1, 512]                  --                        --                        --\n",
       "│    └─Linear: 2-5                                                [1, 512]                  [1, 256]                  131,328                   --                        131,328\n",
       "│    └─ReLU: 2-6                                                  [1, 256]                  [1, 256]                  --                        --                        --\n",
       "│    └─Dropout: 2-7                                               [1, 256]                  [1, 256]                  --                        --                        --\n",
       "│    └─Linear: 2-8                                                [1, 256]                  [1, 128]                  32,896                    --                        32,896\n",
       "│    └─ReLU: 2-9                                                  [1, 128]                  [1, 128]                  --                        --                        --\n",
       "│    └─Dropout: 2-10                                              [1, 128]                  [1, 128]                  --                        --                        --\n",
       "│    └─Linear: 2-11                                               [1, 128]                  [1, 1024]                 132,096                   --                        132,096\n",
       "│    └─ReLU: 2-12                                                 [1, 1024]                 [1, 1024]                 --                        --                        --\n",
       "│    └─Dropout: 2-13                                              [1, 1024]                 [1, 1024]                 --                        --                        --\n",
       "├─level_1_output_layer: 1-3                                       [1, 1024]                 [1, 2]                    --                        --                        --\n",
       "│    └─Linear: 2-14                                               [1, 1024]                 [1, 2]                    2,050                     --                        2,050\n",
       "│    └─Softmax: 2-15                                              [1, 2]                    [1, 2]                    --                        --                        --\n",
       "==============================================================================================================================================================================================\n",
       "Total params: 304,330,626\n",
       "Trainable params: 304,330,626\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 650.54\n",
       "==============================================================================================================================================================================================\n",
       "Input size (MB): 1.35\n",
       "Forward/backward pass size (MB): 1262.07\n",
       "Params size (MB): 1217.32\n",
       "Estimated Total Size (MB): 2480.74\n",
       "=============================================================================================================================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model_level_1, (1, 3, 336, 336), col_names=('input_size', 'output_size', 'num_params', 'kernel_size', 'mult_adds'), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e998ac6-0c6d-42f8-8578-888277bdd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer_level_1(model = model_level_1, level=1, training_dataloader = train_dataloader, validation_dataloader = val_dataloader, epoch = no_epoch, learning_rate = lr, use_gpu = True, opt_method = optimizer, checkpoint_dir = level_1_checkpoint_dir, checkpoint_filename=level_1_model_file, csv_logger = level_1_csvlogger_file  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1bbf59-dadd-4a5d-b2da-9b155fceae5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish initializing...\n",
      "2024-06-11 13:49:51.135425\n",
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                        | 4/699 [06:08<18:02:29, 93.45s/batch, loss=0.693]"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c04bb0b-dcf3-4c17-86f2-0e6a70ae3c70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2.1.0",
   "language": "python",
   "name": "torch_2.1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
