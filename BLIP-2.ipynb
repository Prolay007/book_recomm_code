{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a78c8505-870c-458f-9a24-862f5679666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchinfo\n",
    "import clip  # Import CLIP library\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from level_1_dataloader import image_dataloader\n",
    "from level_1_classifier_1 import level_1_classifier, level_1_model, level_1_output_layer\n",
    "from level_2_classifier import level_2_classifier, level_2_output_layer, level_2_pre_model_concate, level_2_post_model\n",
    "from Trainer_1 import Trainer_level_1, Trainer_level_2\n",
    "from Tester_1 import Tester_level_1, Tester_level_2\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2e5e61-de9d-4c40-99fd-929e5956dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = image_dataloader(csv_file='train_set.xls', root_dir='coverpage/coverpage/')\n",
    "val_set = image_dataloader(csv_file='val_set.xls', root_dir='coverpage/coverpage/')\n",
    "test_set = image_dataloader(csv_file='test_set.xls', root_dir='coverpage/coverpage/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d148ef9f-73b7-4ba5-8434-a7c6d4548b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of training sample 22340\n",
      "no. of validation sample 2804\n",
      "no. of testing sample 2809\n"
     ]
    }
   ],
   "source": [
    "print('no. of training sample', len(train_set))\n",
    "print('no. of validation sample', len(val_set))\n",
    "print('no. of testing sample', len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30be2de2-3d67-45e5-96f8-4ffe5b3c2cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e9a6bc-681d-4d38-a2d0-2ef6982e21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2Model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc994ce-6095-48e0-9fd5-50efa006e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Blip2Model\n",
    "from torchinfo import summary\n",
    "\n",
    "class VisionOnlyModel(torch.nn.Module):\n",
    "    def __init__(self, vision_model):\n",
    "        super(VisionOnlyModel, self).__init__()\n",
    "        self.vision_model = vision_model\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # Extract the class token (first token) from the last hidden state\n",
    "        outputs = self.vision_model(pixel_values=pixel_values).last_hidden_state\n",
    "        class_token_output = outputs[:, 0, :]  # Take the first token\n",
    "        return class_token_output\n",
    "\n",
    "def fine_tune_load_image_model():\n",
    "    # Load the pre-trained BLIP-2 model\n",
    "    blip2_model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "     # Extract the vision model\n",
    "    vision_model = blip2_model.vision_model\n",
    "\n",
    "    # Wrap it in a model class\n",
    "    vision_only_model = VisionOnlyModel(vision_model)\n",
    "\n",
    "    # Enable gradients for the image model's parameters\n",
    "    for p in vision_only_model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return vision_only_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c5f28d-055a-4b43-a5ac-b08d994ecddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260b21e859d84eef88f3ce1906c650c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionOnlyModel(\n",
      "  (vision_model): Blip2VisionModel(\n",
      "    (embeddings): Blip2VisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "    )\n",
      "    (encoder): Blip2Encoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-38): 39 x Blip2EncoderLayer(\n",
      "          (self_attn): Blip2Attention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
      "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Blip2MLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
      "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "FE_model = fine_tune_load_image_model()\n",
    "print(FE_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b22f7ab5-581a-4589-88cd-78a52950ed49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8619c4abb6594eb780f0f336298e6043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vision_model = fine_tune_load_image_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1107d304-bc21-4b49-ad40-072ad287f2ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================================================================================================\n",
      "Layer (type:depth-idx)                                  Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
      "====================================================================================================================================================================================\n",
      "VisionOnlyModel                                         [1, 3, 224, 224]          [1, 1408]                 --                        --                        --\n",
      "├─Blip2VisionModel: 1-1                                 --                        [1, 1408]                 --                        --                        --\n",
      "│    └─Blip2VisionEmbeddings: 2-1                       [1, 3, 224, 224]          [1, 257, 1408]            363,264                   --                        --\n",
      "│    │    └─Conv2d: 3-1                                 [1, 3, 224, 224]          [1, 1408, 16, 16]         829,312                   [14, 14]                  212,303,872\n",
      "│    └─Blip2Encoder: 2-2                                --                        [1, 257, 1408]            --                        --                        --\n",
      "│    │    └─ModuleList: 3-2                             --                        --                        984,756,864               --                        --\n",
      "│    └─LayerNorm: 2-3                                   [1, 257, 1408]            [1, 257, 1408]            2,816                     --                        2,816\n",
      "│    └─LayerNorm: 2-4                                   [1, 1408]                 [1, 1408]                 (recursive)               --                        2,816\n",
      "====================================================================================================================================================================================\n",
      "Total params: 985,952,256\n",
      "Trainable params: 985,952,256\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.20\n",
      "====================================================================================================================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 1288.73\n",
      "Params size (MB): 3942.36\n",
      "Estimated Total Size (MB): 5231.69\n",
      "====================================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================================================================================================\n",
       "Layer (type:depth-idx)                                  Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "====================================================================================================================================================================================\n",
       "VisionOnlyModel                                         [1, 3, 224, 224]          [1, 1408]                 --                        --                        --\n",
       "├─Blip2VisionModel: 1-1                                 --                        [1, 1408]                 --                        --                        --\n",
       "│    └─Blip2VisionEmbeddings: 2-1                       [1, 3, 224, 224]          [1, 257, 1408]            363,264                   --                        --\n",
       "│    │    └─Conv2d: 3-1                                 [1, 3, 224, 224]          [1, 1408, 16, 16]         829,312                   [14, 14]                  212,303,872\n",
       "│    └─Blip2Encoder: 2-2                                --                        [1, 257, 1408]            --                        --                        --\n",
       "│    │    └─ModuleList: 3-2                             --                        --                        984,756,864               --                        --\n",
       "│    └─LayerNorm: 2-3                                   [1, 257, 1408]            [1, 257, 1408]            2,816                     --                        2,816\n",
       "│    └─LayerNorm: 2-4                                   [1, 1408]                 [1, 1408]                 (recursive)               --                        2,816\n",
       "====================================================================================================================================================================================\n",
       "Total params: 985,952,256\n",
       "Trainable params: 985,952,256\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.20\n",
       "====================================================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 1288.73\n",
       "Params size (MB): 3942.36\n",
       "Estimated Total Size (MB): 5231.69\n",
       "===================================================================================================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(\n",
    "    vision_model, \n",
    "    input_size=(1, 3, 224, 224), \n",
    "    col_names=('input_size', 'output_size', 'num_params', 'kernel_size', 'mult_adds'), \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7d93d7e-ba4e-47ac-b2ac-dc06d6e88c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ./checkpoints/BLIP-2/level_1/ is ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "used_model = 'BLIP-2'\n",
    "used_model_feature_size = 1408\n",
    "level_1_checkpoint_dir = './checkpoints/' + used_model + '/level_1/'\n",
    "level_1_model_file = 'model.pth'\n",
    "level_1_csvlogger_file = 'log.csv'\n",
    "level_1_weights_path = os.path.join(level_1_checkpoint_dir, level_1_model_file)\n",
    "feature_size_extract_from_level_1 = 64\n",
    "no_epoch = 10\n",
    "lr = 0.00001\n",
    "optimizer = 'AdamW'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(level_1_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Print a message to confirm the directory creation (optional)\n",
    "print(f\"Directory {level_1_checkpoint_dir} is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "702014e1-0d68-4813-beb4-163823ccbbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_model_feature_size = 1408  # Example value, adjust based on your model\n",
    "feature_size_extract_from_level_1 = 1408 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d143c2a1-f885-49b6-adf7-60f64e409cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and prepare model components\n",
    "classifier_level_1 = level_1_classifier(feature_size=used_model_feature_size, feature_size_extract_from_level_1=feature_size_extract_from_level_1)\n",
    "output_layer_level_1 = level_1_output_layer(feature_size_extract_from_level_1=feature_size_extract_from_level_1, no_class=2)\n",
    "model_level_1 = level_1_model(FE_model, classifier_level_1, output_layer_level_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb05e440-1e8c-48cb-b78d-3999ee047012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================================================================================================\n",
      "Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
      "=========================================================================================================================================================================================\n",
      "level_1_model                                                [1, 3, 224, 224]          [1, 2]                    --                        --                        --\n",
      "├─VisionOnlyModel: 1-1                                       [1, 3, 224, 224]          [1, 1408]                 --                        --                        --\n",
      "│    └─Blip2VisionModel: 2-1                                 --                        [1, 1408]                 --                        --                        --\n",
      "│    │    └─Blip2VisionEmbeddings: 3-1                       [1, 3, 224, 224]          [1, 257, 1408]            1,192,576                 --                        212,303,872\n",
      "│    │    └─Blip2Encoder: 3-2                                --                        [1, 257, 1408]            984,756,864               --                        984,756,864\n",
      "│    │    └─LayerNorm: 3-3                                   [1, 257, 1408]            [1, 257, 1408]            2,816                     --                        2,816\n",
      "│    │    └─LayerNorm: 3-4                                   [1, 1408]                 [1, 1408]                 (recursive)               --                        2,816\n",
      "├─level_1_classifier: 1-2                                    [1, 1408]                 [1, 1408]                 --                        --                        --\n",
      "│    └─Linear: 2-2                                           [1, 1408]                 [1, 512]                  721,408                   --                        721,408\n",
      "│    └─ReLU: 2-3                                             [1, 512]                  [1, 512]                  --                        --                        --\n",
      "│    └─Dropout: 2-4                                          [1, 512]                  [1, 512]                  --                        --                        --\n",
      "│    └─Linear: 2-5                                           [1, 512]                  [1, 256]                  131,328                   --                        131,328\n",
      "│    └─ReLU: 2-6                                             [1, 256]                  [1, 256]                  --                        --                        --\n",
      "│    └─Dropout: 2-7                                          [1, 256]                  [1, 256]                  --                        --                        --\n",
      "│    └─Linear: 2-8                                           [1, 256]                  [1, 128]                  32,896                    --                        32,896\n",
      "│    └─ReLU: 2-9                                             [1, 128]                  [1, 128]                  --                        --                        --\n",
      "│    └─Dropout: 2-10                                         [1, 128]                  [1, 128]                  --                        --                        --\n",
      "│    └─Linear: 2-11                                          [1, 128]                  [1, 1408]                 181,632                   --                        181,632\n",
      "│    └─ReLU: 2-12                                            [1, 1408]                 [1, 1408]                 --                        --                        --\n",
      "│    └─Dropout: 2-13                                         [1, 1408]                 [1, 1408]                 --                        --                        --\n",
      "├─level_1_output_layer: 1-3                                  [1, 1408]                 [1, 2]                    --                        --                        --\n",
      "│    └─Linear: 2-14                                          [1, 1408]                 [1, 2]                    2,818                     --                        2,818\n",
      "│    └─Softmax: 2-15                                         [1, 2]                    [1, 2]                    --                        --                        --\n",
      "=========================================================================================================================================================================================\n",
      "Total params: 987,022,338\n",
      "Trainable params: 987,022,338\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.20\n",
      "=========================================================================================================================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 1288.75\n",
      "Params size (MB): 3946.64\n",
      "Estimated Total Size (MB): 5235.99\n",
      "=========================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prolay/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================================================================================================\n",
       "Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "=========================================================================================================================================================================================\n",
       "level_1_model                                                [1, 3, 224, 224]          [1, 2]                    --                        --                        --\n",
       "├─VisionOnlyModel: 1-1                                       [1, 3, 224, 224]          [1, 1408]                 --                        --                        --\n",
       "│    └─Blip2VisionModel: 2-1                                 --                        [1, 1408]                 --                        --                        --\n",
       "│    │    └─Blip2VisionEmbeddings: 3-1                       [1, 3, 224, 224]          [1, 257, 1408]            1,192,576                 --                        212,303,872\n",
       "│    │    └─Blip2Encoder: 3-2                                --                        [1, 257, 1408]            984,756,864               --                        984,756,864\n",
       "│    │    └─LayerNorm: 3-3                                   [1, 257, 1408]            [1, 257, 1408]            2,816                     --                        2,816\n",
       "│    │    └─LayerNorm: 3-4                                   [1, 1408]                 [1, 1408]                 (recursive)               --                        2,816\n",
       "├─level_1_classifier: 1-2                                    [1, 1408]                 [1, 1408]                 --                        --                        --\n",
       "│    └─Linear: 2-2                                           [1, 1408]                 [1, 512]                  721,408                   --                        721,408\n",
       "│    └─ReLU: 2-3                                             [1, 512]                  [1, 512]                  --                        --                        --\n",
       "│    └─Dropout: 2-4                                          [1, 512]                  [1, 512]                  --                        --                        --\n",
       "│    └─Linear: 2-5                                           [1, 512]                  [1, 256]                  131,328                   --                        131,328\n",
       "│    └─ReLU: 2-6                                             [1, 256]                  [1, 256]                  --                        --                        --\n",
       "│    └─Dropout: 2-7                                          [1, 256]                  [1, 256]                  --                        --                        --\n",
       "│    └─Linear: 2-8                                           [1, 256]                  [1, 128]                  32,896                    --                        32,896\n",
       "│    └─ReLU: 2-9                                             [1, 128]                  [1, 128]                  --                        --                        --\n",
       "│    └─Dropout: 2-10                                         [1, 128]                  [1, 128]                  --                        --                        --\n",
       "│    └─Linear: 2-11                                          [1, 128]                  [1, 1408]                 181,632                   --                        181,632\n",
       "│    └─ReLU: 2-12                                            [1, 1408]                 [1, 1408]                 --                        --                        --\n",
       "│    └─Dropout: 2-13                                         [1, 1408]                 [1, 1408]                 --                        --                        --\n",
       "├─level_1_output_layer: 1-3                                  [1, 1408]                 [1, 2]                    --                        --                        --\n",
       "│    └─Linear: 2-14                                          [1, 1408]                 [1, 2]                    2,818                     --                        2,818\n",
       "│    └─Softmax: 2-15                                         [1, 2]                    [1, 2]                    --                        --                        --\n",
       "=========================================================================================================================================================================================\n",
       "Total params: 987,022,338\n",
       "Trainable params: 987,022,338\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.20\n",
       "=========================================================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 1288.75\n",
       "Params size (MB): 3946.64\n",
       "Estimated Total Size (MB): 5235.99\n",
       "========================================================================================================================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model_level_1, (1, 3, 224, 224), col_names=('input_size', 'output_size', 'num_params', 'kernel_size', 'mult_adds'), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a61e0327-2509-4e68-a1b1-519486355c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer_level_1(model = model_level_1, level=1, training_dataloader = train_dataloader, validation_dataloader = val_dataloader, epoch = no_epoch, learning_rate = lr, use_gpu = True, opt_method = optimizer, checkpoint_dir = level_1_checkpoint_dir, checkpoint_filename=level_1_model_file, csv_logger = level_1_csvlogger_file  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a11d8f0-6ac7-4fd2-8a9d-5e5874def71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish initializing...\n",
      "2024-06-11 14:41:18.942663\n",
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████▋             | 234/699 [7:47:28<15:28:56, 119.86s/batch, loss=0.353]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Trainer_1.py:487\u001b[0m, in \u001b[0;36mTrainer_level_1.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# if self.level == 1:\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m avg_loss, train_precision, train_recall, train_f1, train_acc, train_ba, train_spec, train_hm, avg_vloss, val_precision, val_recall, val_f1, val_acc, val_ba, val_spec, val_hm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch_level_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLOSS train \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, train_precision_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, train_recall_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, train_f1 \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, train_acc_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, train_ba_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, train_spec_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, train_hm_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, val_loss \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, val_precision_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, val_recall_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, val_f1 \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, val_acc_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, val_ba_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, val_spec_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, val_hm_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(avg_loss, train_precision, train_recall, train_f1, train_acc, train_ba, train_spec, train_hm, avg_vloss, val_precision, val_recall, val_f1, val_acc, val_ba, val_spec, val_hm))\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# training_range.set_description(\"Epoch %d | train loss: %f | train_precision: %f | train_recall: %f | train_f1: %f| val loss: %f | val_precision: %f | val_recall: %f | val_f1: %f\" % (epoch, avg_loss, train_precision, train_recall, train_f1, avg_vloss, val_precision, val_recall, val_f1))\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Trainer_1.py:418\u001b[0m, in \u001b[0;36mTrainer_level_1.train_one_epoch_level_1\u001b[0;34m(self, epoch_number)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_one_epoch_level_1\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch_number):\n\u001b[0;32m--> 418\u001b[0m \tavg_loss, train_precision, train_recall, train_f1, train_acc, train_ba, train_spec, train_hm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \tavg_vloss, val_precision, val_recall, val_f1, val_acc, val_ba, val_spec, val_hm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_one_epoch(epoch_number)\n\u001b[1;32m    421\u001b[0m \t\u001b[38;5;66;03m# print(avg_loss,train_precision,train_recall,train_f1)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Trainer_1.py:292\u001b[0m, in \u001b[0;36mTrainer_level_1.train_one_epoch\u001b[0;34m(self, epoch_index)\u001b[0m\n\u001b[1;32m    288\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 292\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(outputs,labels)\n\u001b[1;32m    296\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/level_1_classifier_1.py:88\u001b[0m, in \u001b[0;36mlevel_1_model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m \tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mVisionOnlyModel.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Extract the class token (first token) from the last hidden state\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     13\u001b[0m     class_token_output \u001b[38;5;241m=\u001b[39m outputs[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# Take the first token\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m class_token_output\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/transformers/models/blip_2/modeling_blip_2.py:576\u001b[0m, in \u001b[0;36mBlip2VisionModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    574\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[0;32m--> 576\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    584\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_layernorm(last_hidden_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/transformers/models/blip_2/modeling_blip_2.py:514\u001b[0m, in \u001b[0;36mBlip2Encoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    507\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    508\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    509\u001b[0m         hidden_states,\n\u001b[1;32m    510\u001b[0m         attention_mask,\n\u001b[1;32m    511\u001b[0m         output_attentions,\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 514\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/transformers/models/blip_2/modeling_blip_2.py:286\u001b[0m, in \u001b[0;36mBlip2EncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    284\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    285\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm2(hidden_states)\n\u001b[0;32m--> 286\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m residual\n\u001b[1;32m    290\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/transformers/models/blip_2/modeling_blip_2.py:245\u001b[0m, in \u001b[0;36mBlip2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    243\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(hidden_states)\n\u001b[1;32m    244\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(hidden_states)\n\u001b[0;32m--> 245\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_2.1.0/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce28f307-9eb5-4f2e-9c1d-6cd785ca80f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2.1.0",
   "language": "python",
   "name": "torch_2.1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
